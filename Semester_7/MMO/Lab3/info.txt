1-ая часть:
	- Метод наименьших квадратов(не всегда работает из-за невозможности получить обратную матрицу в случае, если X * X транспонированную будет вырожденной,
		также из-за необходимости считать обратную матрицу вычислительно сложен)
	- Метод градиентного спуска(дает то же значение ошибки, но лучше. Итеративный)
	
2-ая часть:
	- Нормализация: приведение признаков к к масштабу в диапазоне: [0; 1]
	- Стандартизация: заключается в получении своего рода значения сдвига каждого признака от среднего
	- В стохастическом градиентом спуске: Стремление к оптимизации процесса привело к появлению 
		стохастического градиентного спуска (Stochastic gradient descent, SGD). 
		Идея его основана на том, что на одной итерации мы вычитаем не вектор градиента, вычисленный по всей выборке, 
		а вместо этого случайно выбираем один объект из обучающей выборки  xi  и вычисляем градиент только на этом объекте, 
		то есть градиент только одного слагаемого в функционале ошибки и вычитаем именно этот градиент из текущего приближения вектора весов.
	- Преимущество стохастического: гораздо более быстрое вычисление на каждом шаге + отсутствие необходимости хранить выборку в памяти,
		что позволяет работать с очень большими объемами данных
	- Недообучение несет за собой плохое качество на обучении и на новых данных, а переобучение - хорошее качество на обучении и плохое на новых данных.
		Пример их кода с кубической зависимостью, для которой создали данные, добавив шум, 
			а потом почти идеально описали функцию 8-ой степени, описывающую эти данные - пример переобучения.
	- Одним из знаков, что произошло переобучение модели, или мерой сложности является получение больших по модулю весов при признаках. 
	- L1 и L2 нужны для избегания переобучения, но делают это по-разному
	